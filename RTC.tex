% !Mode:: "TeX:UTF-8"
\documentclass[10pt,journal]{IEEEtran}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{comment}
\usepackage{multirow}

\graphicspath{figures/}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}

\begin{document}
\title{A Delay and Backlog Model for Priority-Aware Networks-on-Chip (NoC)}

\author{\IEEEauthorblockN{Baoliang Li}}
%\onecolumn
\maketitle

\begin{abstract}
Networks-on-Chip (NoC) is a key component for modern Chip-MultiProcessors(CMPs) and System-on-Chip (SoC). Among all the implementation alternatives, priority-aware wormhole-switched NoC is promising to meet the rigorous requirements of on-chip latency-aware communication, e.g. cache coherent protocol and multimedia applications. The end-to-end latency and buffer requirement analysis are very important for the development of real-time applications on this platform. In this paper, we propose a Real-Time Calculus (RTC) based latency and backlog analysis model to achieve this goal. Our model is topology-insensitive, it takes as input the scheduling network model and the application traffic characterization, and gives the end-to-end latency and backlog bound for each traffic flow, which enables the fast performance evaluation and buffer optimization. Experiment results demonstrate the effectiveness and tightness of our model. In addition, further comparisons with other theoretical models also indicates that, our method outperforms the existing methods while the tightness of the derived delay and backlog bound are considered.
\end{abstract}
\begin{IEEEkeywords}
Networks-on-Chip (NoC), priority-aware, wormhole switch, real-time calculus, delay and backlog bound
\end{IEEEkeywords}

\section{Introduction}
Conventional interconnection paradigms, e.g. bus, ring and point-to-point links, are not able to meet the strict and complex communication requirements of modern large scale Chip-MultiProcessors (CMPs) and System-on-Chip (SoC). As an alternative, Networks-on-Chip (NoC) is proposed to provide better scalability, higher power efficiency, and low-latency global communication, etc. As a key component of CMPs and SoC, NoC must be well designed to meet the rigorous requirements on end-to-end latency, buffer constraint, throughput and power, etc. Although various proposals have been emerged, with each focusing on improving different performance metrics of on-chip communication, most of the existing research on NoC are focusing on the improvement of average performance of on-chip wormhole-switched network, and simulation is the widely used performance evaluation method. Whereas, there also exists lots of on-chip applications, which are sensitive to the worst-case or real-time communication performance of NoC, e.g. cache coherent protocol \cite{Bolotin2007}\cite{5951888} and multimedia application \cite{ostermann2004video}. How to design the on-chip communication infrastructure for these applications and analyze its feasibility are of big challenges for the researchers.

To meet the rigorous Quality-of-Service (QoS) requirement, various special hardware implementations have been proposed, e.g. Time-Division Multiplexing-Access (TDMA) \cite{GoDR05} and time-triggered switch \cite{4617280}, etc. Although provide strict real-time communication guarantee, the average performance and resource utilization of these proposals are very poor. In contrast, wormhole-switched NoC is widely used in on-chip network due to its simplicity and high-efficiency. Thus, providing real-time communication support on the conventional wormhole-switched NoC to meet both real-time and non-real-time communication requirements is the most promising solution. To achieve this goal, a special scheduling policy (e.g. DifServ \cite{1411140} or priority-aware implementation \cite{Shi:2008:RCA:1397757.1397996}\cite{708526}\cite{627905}) or flow control mechanism (e.g. \cite{Li199649}\cite{707545}) should be implemented. For all these wormhole-switch based real-time communication proposals, a key step before their adoption as a platform of real-time applications is the analysis of the worst-case communication latency for all the real-time flows, to guarantee that the deadline of each flow can be met. In addition, an effective buffer estimation approach is also needed to optimize the buffer allocation under real-time constraint.

A reliable worst-case analysis is crucial for the application of wormhole-switched NoC, because an over optimistic estimation will lead to a wrong implementation, while an over pessimistic estimation will make the utilization of on-chip resource very low. The conventional simulation based method might not be competent for the worst-case analysis, this is because the worst-case scenarios are hard to be captured by simulation. As an alternative, the mathematical approach can establish the relationship between performance metrics and design parameters in a very short time, and giving the worst-case performance immediately. For the worst-case analysis of fixed-priority wormhole-switched on-chip networks, Flow-Level Analysis (FLA) \cite{Shi:2008:RCA:1397757.1397996}, Link-Level Analysis (LLA) \cite{73}\cite{189} and network calculus \cite{Qian489900} based analysis are widely used. Both FLA and LLA have their roots in the classic scheduling theory, which assume that, the traffic flows are strictly periodic, and the packet transmission delay is less than the packet inter-arrival time. In addition, they both assume that, the input buffer of wormhole-switched NoC is sufficient large, so that the back-pressure caused by flow control can be ignored. Network calculus overcomes these limitations by allowing the packet arrival at arbitrary pattern and arbitrary inter-arrival time. By applying the advanced sub-additive closure operator in dioid algebra, the large buffer assumption can also be eliminated.

However, we found that, the network calculus based latency bound can be further improved, this is because, the network calculus based method \cite{Qian489900} ignored the maximal service capacity of on-chip routers, which has significant impact on the output arrival curve of traffic flow served by a specific router. The over estimated output arrival curve will further leads to a looser service curve left for other flows, and the over pessimistic service curve will finally leads to a looser performance bound for the low-priority flows. We will further demonstrate this phenomenon in Section \ref{experiments}. To overcome this shortcoming of network calculus, we adopt the real-time calculus (RTC) to improve these delay bound in this paper. Compared with network calculus, the RTC use the maximal service curve and minimal arrival curve to limit the output arrival curve and left service curve, which usually leads to a tighter performance bound. The advantages of RTC will be also demonstrated in section \ref{experiments}. In this paper, we build an end-to-end latency model for the wormhole-switched, credit-based flow controlled on-chip networks with RTC, and an algorithm is proposed to automatically compute the end-to-end performance bound for each flows. The main contribution of this paper is two folded: (1) we propose an end-to-end performance analysis algorithm, compared with the existing methods, e.g. LLA \cite{73}\cite{189} and network calculus \cite{Qian489900}, it can gives better performance bound. The output of this algorithm can be used for the IP core mapping, task mapping, routing selection, or NoC parameters configuration; (2) we propose an RTC based buffer optimization algorithm for the application-specific NoC the reduce the buffer size under strict deadline constraint.

The rest of this paper is organized as follows: we present the existing real-time communication proposals and its related performance analysis method in Section \ref{related}. In Section \ref{model}, the basic assumptions on wormhole switched on-chip network and RTC theory is introduced. The detailed modeling process is presented in Section \ref{modeling}, and we present the experiment results and comparison with other mathematical methods in Section \ref{experiments}. Finally, we summarize our paper in Section \ref{conclusion}.

\section{Related Work}\label{related}
Since introduced in 2001 \cite{DaTo01}, various NoC proposals have been emerged to meet different on-chip communication requirements. Different applications have different communication requirement, and the main requirements posed to NoC by on-chip applications are latency and bandwidth. To meet these demand, NoC are designed to be either either best-effort or guaranteed-service, depending on the hardware cost and application fields. The worst case analysis for these two categories is slightly different. Synchronous Data Flow (SDF) graph \cite{poplavko2003task} and Deterministic Network Calculus \cite{qian2009analysis} (DNC) have been presented to model the worst case performance bound of best-effort NoC. The former method assumes the traffic flow to be periodical, and the latter one eliminates this constraint to allow the traffic to be arbitrary patterns, which extends applicable of the method. In \cite{qian2009analysis}, the authors build an analytical performance model with network calculus taking the various contention and flow control into consideration. This result is extended in \cite{Du:2012:WPA:2380445.2380469}, where the traffic splitter was proposed to support the multi-path routing polices. Another method is presented in \cite{Lee:2003:RWC:846077.846083} to compute the worst-case latency for conventional wormhole switched network, and a real-time Wormhole Channel Feasibility Checking (WCFC) algorithm is proposed. This research is further extended to calculate the bandwidth and latency bound in \cite{6109240}, and used for topology synthesizing of best-effort NoC in \cite{EPFL-ARTICLE-186879}. For more details about the mathematic modeling of best-effort NoC please refer to \cite{Kiasari:2013:MFP:2480741.2480755}.

For the implementation of service-guaranteed NoC, a simple and effective solution to provide differential service for different applications is classify these applications into several service classes, each with different priorities, and the network provides services according to the priority of each class. Representative implementations of this idea include QNoC \cite{BCGK04}, fixed-priority NoC \cite{5685465} and {{\AE}thereal} \cite{GoDR05}\cite{RiGW01} etc. Although all the analytical methods mentioned above aim for the worst-case analysis of conventional NoC, when they are adopted to the priority-aware NoC, the obtained performance bound is very conservative, especially for the high priority flows. This is because, these methods do not take the priority-aware scheduling into consideration. Thus, new methods should be developed for the real-time communication analysis. In \cite{LuJS05}, contention tree was proposed to analyze the feasibility of real-time traffic delivered by wormhole-switched NoC. It improves the previous results , e.g. lumped link model \cite{707545} and dependency graph model\cite{708526}, by allowing the concurrent link usage. This is similar as the the Link Level Analysis \cite{73}\cite{holisticNoC13} (LLA), which improved the Flow-Level-Analysis (FLA) proposed in \cite{Shi:2008:RCA:1397757.1397996}. The main difference between FLA and LLA is that, FLA treats the entire route of a flow as a whole, while the LLA treats each link segment separately. Link-level analysis can provide much accurate results than FLA \cite{73}, this is because in the LLA, the latency on specific link only related with the inference other flows posed on the target flow on previous links. The main drawback of FLA and LLA is that, these method require the traffic arrival periodically, and the packet inter-arrival time must greater than the end-to-end transmission latency, and the previous research based on these two method assumes that, the buffer at each routers is sufficient large, so that the feedback caused by flow-control can be ignored. To overcome these shortcomings, network calculus \cite{Qian489900} was used to analyze the worst case latency of priority-aware NoC.

But we found that, the network calculus results can be further improved if we take the maximum service curve of each router and minimum arrival curve of each flow into consideration, this is because the maximal service curve and minimum arrival curve can limit the output arrival curve, which leads to a much tighter left service curve for the low priority flow. (to explain the reason, please refer to Theorem 1.6.2 in \cite{Boudec2001Network}). To overcome this shortcoming, we adopt the Real-Time Calculus (RTC) \cite{ThCN00}\cite{1253607} originally used for real-time task scheduling analysis to compute the worst-case end-to-end performance of priority-aware wormhole-switched NoC. Real-Time Calculus is extension of network calculus by integrating the maximum service curve and lower arrival curve into the network calculus theory. Due to the theoretical results is usually very tight, it has been widely used in the modeling and analysis of network processor \cite{1253838}, CAN \cite{4617308}, FlexRay \cite{Chokshi:2010:PAF:1774088.1774162}\cite{Hagiescu:2007:PAF:1278480.1278554} and DSP systems \cite{thiele2005performance}, etc. To ease the application of Network Calculus, an RTC toolbox \cite{rtc} has also been implemented to support the numerical calculation.

\section{Preliminaries}\label{model}
\subsection{Basic Assumptions}
For the detailed description of NoC architecture, please refer to \cite{jerger2009chip}\cite{DaTo04}. In this paper, we consider the same network model as in \cite{627905}\cite{Shi:2008:RCA:1397757.1397996}\cite{707545}\cite{73}, each router has 5 pipeline stages, i.e. Buffer-Write (BW), Route Computation (RC), VC Allocation (VA), Switch Allocation (SA), Switch Traversal (ST) and Link Traversal (LT). IP core communication with each other by transmitting packets, each packet is composed of a header flit and several non-header flits optionally. Each header flit should traverse all the five stages to find a path and reserve buffer space for the follow non-header flits, and non-header flits skip the RC and VA stage since the routine and VC have been determined by header flit. To ensure the predication of packet, we assume the NoC adopts deterministic routing polices and the switch allocator is priority aware. If multiple flits from different input ports or different VCs of the same input port contend for the same output port, the priority-aware switch allocator will only grant the flit with highest priority. Flits from a lower priority can transmit a flit if and only if there are no flits from higher priority in the input buffer or the flits with higher priority are self-blocked due to the insufficiency of VC buffer at downstream router. The packet of high priority can also preempt the transmission of packets with low priority. In addition, we also assume that, the buffer depth of each VC is finite, and credit-based flow control is adopted between each router. Although we focus on the 5-stage router, our method can be easily adopted to the speculation-based router, e.g. routers with only 2 or 3 stages, the only difference is the initial delay of our model. To simplify of our analysis, we also assume that, the entire chip use synchronous design, the frequency and period of clock are $f$ and $T$, respectively. Our method can also be applied to analyze Global Asynchronous Local Synchronous (GALS) NoC without any modification, because the router located in different voltage-frequency islands can communicate with a half cycle fixed-latency synchronizer \cite{5476986}.

Our model is topology independent, but to demonstrate the basic idea of our method, we take the mesh topology as an example, as shown in Fig. \ref{topology}. There are 4 traffic flows in the network, i.e. $f_1$, $f_2$, $f_3$ and $f_4$. A flow is a sequence of packets following the same transmission path with the same source and destination address. We must emphasize that, although there is only four flows in the network, it is sufficient to demonstrate our idea, and our method can handle more traffic flows efficiently. To ensure the low latency transmission for real-time traffic flows, we have to assign higher priorities to these flows. Denote by $P_i$ the priority of flow $f_i$, in the example shown in Fig. \ref{topology}, we assume $P_1=P_4=3$, $P_2=2$ and $P_3=1$, and a higher value indicates a higher priority. Our method extends the existing methods \cite{73}\cite{Qian489900} which assumes per-flow priority, to allows multiple flows share a same priority, e.g. both $f_1$ and $f_4$ in Fig. \ref{topology} are assigned the highest priority. Flits from different flows with the same priority are served in round-robin fashion, and the flits of the same flow were served in FIFO order. As the minimum transmission unit in NoC is flit and a higher priority packet can preempt the transmission of a lower priority packet, the NoC architecture considered in this paper is flit-level preemptive \cite{Lee:2003:RWC:846077.846083}. Our results can be used to determine the worst-case latency and backlog bound of each flow. If all the packets of one flow can be transmitted within their deadlines, the flow is schedulable.
\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.9]{figures/mesh.pdf}\\
  \caption{A Mesh NoC with 4 Traffic Flow}\label{topology}
\end{figure}

\subsection{Introduction to Real-Time Calculus}
Real-time calculus is an extension of network calculus \cite{ThCN00}\cite{1253607}, by adding the upper service curve and lower arrival curve to describe the maximal service capacity and lower arrival rate. It is the mathematical basis of Modular Performance Analysis (MPA) \cite{Wandeler2006System} technique used for real-time task scheduling. Here we only present the definition of RTC arrival curve and service curve, for more details about this theory, please refer to \cite{ThCN00}\cite{1253607}.
\begin{definition}[Real-Time Arrival Curve]
Let $R[s,t)$ denote the number of events that arrived in the time interval $[s,t)$. The lower bound and upper bound on $R[s,t)$ is called the lower arrival curve $\alpha^l$ and upper arrival curve $\alpha^u$ which satisfy
$$\alpha^l(t-s)\leq R[s,t)\leq \alpha^u(t-s),\forall s<t$$
where $\alpha^l(0)=\alpha^u(0)=0$. The RTC arrival curve for $R$ is denoted as $<\alpha^l,\alpha^u>$ for short.
\end{definition}

From the definition, we find that the upper arrival curve (corresponding to the arrival curve of network calculus \cite{Boudec2001Network}) and lower arrival curve are used to characterize the upper and lower bound of arrival event with any interval $\Delta$.

\begin{definition}[Real-Time Service Curve]
Let $S[s,t)$ denote the total number of events that can be processed by the system in the time interval $[s,t)$. The lower bound and upper bound on $S[s,t)$ is called the lower service curve $\alpha^l$ and upper service curve $\alpha^u$ which satisfy
$$\beta^l(t-s)\leq S[s,t)\leq \beta^u(t-s),\forall s<t$$
where $\beta^l(0)=\beta^u(0)=0$. The RTC service curve for $S$ is denoted as $<\beta^l,\beta^u>$ for short.
\end{definition}

From the definition of RTC service curve, we find that the upper and lower service curve are corresponding to the maximal service curve and service curve in network calculus \cite{Boudec2001Network}, respectively. Thus, the concatenation theorem (see Theorem 1.46 and Theorem 1.6.1 in \cite{Boudec2001Network}) for service curve and maximal service curve can also be applied to RTC service curve. In this paper, event is refer to the arrival of flits. In this paper, we will use the discrete time RTC arrival curve and service curve to characterize the system, this is because the minimal time unit in the wormhole-switched NoC is clock period $T$. If we obtain the arrival curve $<\alpha^l,\alpha^u>$ of a specific traffic flow and the service curve $<\beta^l,\beta^u>$ provided to this flow, we can get the output arrival curve of this flow and leftover service curve for the other flows with the following equations:
\begin{equation}\label{alphau}
\alpha^{u^\prime}=\lceil\min\{(\alpha^u\otimes\beta^u)\oslash\beta^l,\beta^u\}\rceil
\end{equation}
\begin{equation}\label{alphal}
\alpha^{l^\prime}=\lfloor\min\{(\alpha^l\oslash\beta^u)\otimes\beta^l,\beta^l\}\rfloor
\end{equation}
\begin{equation}\label{betau}
\bar{\beta}^{u^\prime}=\max\{(\bar{\beta}^u-\bar{\alpha}^l)\bar{\oslash}0,0\}
\end{equation}
\begin{equation}\label{betal}
\bar{\beta}^{l^\prime}=(\bar{\beta}^l-\bar{\alpha}^u)\bar{\otimes}0
\end{equation}
where $\lceil\cdot\rceil$, $\lfloor\cdot\rfloor$ are the ceiling operator and flooring operator, and $\otimes$, $\oslash$, $\bar{\otimes}$, $\bar{\oslash}$ are corresponding to the min-plus convolution, de-convolution, and max-plus convolution and de-convolution \cite{Boudec2001Network}, respectively.

After we obtained the arrival curve $<\alpha^l_{f_i},\alpha^u_{f_i}>$ of traffic flow $f_i$ and the service curve $<\beta_{R_j,f_i}^l,\beta_{R_j,f_i}^u>$ of each router $R_j$ on the path of $f_i$, we can obtain the end-to-end latency and backlog bound by the following equations,
\begin{equation}\label{delay}
Delay(f)=H(\alpha^u_{f_i},\beta^l_{R_1}\otimes\beta^l_{R_2}\otimes\cdots\otimes\beta^l_{R_N}).
\end{equation}
\begin{equation}\label{backlog}
Backlog(f)=V(\alpha^u_{f_i},\beta^l_{R_1}\otimes\beta^l_{R_2}\otimes\cdots\otimes\beta^l_{R_N}).
\end{equation}
where $H(\cdot,\cdot)$ and $V(\cdot,\cdot)$ mean the maximal vertical and horizontal deviation, respectively.

\section{Real-Time Calculus based Performance Model}\label{modeling}
Before carry out the performance analysis with RTC, we should first build the traffic and router model. In this paper, we will use the discrete time service curve and arrival curve to characterize the router and traffic, these models can be input to the RTC toolbox \cite{rtc} to compute the end-to-end latency automatically. While applying the RTC theory to evaluation the performance of priority-aware wormhole-switched NoC, the following four aspects should be considered: (1) Only header flit need to be processed by RC and VA stage, the subsequent flits of a packet just follow the decision of header flit. We need specific mechanism to characterize the service provided to header and non-header flits in a unified way, so that we can simplify our RTC model; (2) Our model allows priority sharing between flows, thus, the leftover service curve provided to lower priority flows can only be derived after all the flows with higher priority have been serviced; (3) Due to the on-chip buffer limitation, credit-based flow control is used as a back-pressure mechanism to prevent buffer overflow. Before analysis the end-to-end performance with Eq.(\ref{delay}) and Eq.(\ref{backlog}), we should first break the cyclic-dependence caused by flow control; (4) To guarantee the tightness of our theoretical bound and reduce the computation complexity, we should collapse all the collapsible subpaths. These four issues are discussed in the follow subsections.

\subsection{Traffic Model}\label{traffic}
In a CMP or SoC system interconnected with NoC, the communication between each pair of cores was realized by transmitting packets. Each packet is further divided into flits, which is the minimum transmission unit in wormhole-switched NoC. Denote the arrival curve $<\alpha^l(\Delta),\alpha^u(\Delta)>$ of flow as the minimum and maximum amount of flits within $\Delta$ time unit. Here, we provide two ways to obtain this arrival curve: (1) We can extract the arrival curve from the communication trace generated by a real CMP or SoC system with the sliding window method \cite{1253607}. This method analyze the time series of flits by the follow way: for each window size $\Delta$, it try to find the maximal and minimal number of arrived flits in the trace, denoted by $\alpha^l(\Delta)$ and $\alpha^u(\Delta)$, respectively. We need to mention that, the obtained arrival curve obtained by this method might not be periodic; (2) if we know the inter-arrival time of packets in a flow, we can also get the arrival curve directly. For example, suppose all the packets have the same length $L$ and arrived periodically with period $P$. Then, we can obtain the service curve of this flow by simply amplifying the arrival curve for periodic event stream provided in \cite{1253607} at $y$-axis with a factor $L$, as shown in Fig. \ref{ac}. The alert readers would notice that, the flit arrival curve obtained by this way is the same as the packet arrival curve $\mathcal{P}^L(\alpha)$. Thus, this traffic model can be used to compute the end-to-end packet latency, please refer to Theorem 1.7.1 in \cite{Boudec2001Network} for explanation and more details.
\begin{figure}
  \centering
  \includegraphics[scale=0.5]{figures/AC.pdf}
  \caption{Arrival Curve for Periodically Arriving Traffic}\label{ac}
\end{figure}

\subsection{Router Model}\label{router}
While modeling the routers with RTC, we can analyze the data path stage by stage, and find the service curve for each stage. Then, the service curve provided by the router to each flow can be obtained by concatenating all the service curves of these five stages. This is significantly different with the existing network calculus based model \cite{Qian2010Analysis,Qian489900}, where they view the entire router as a whole. The advantage of our method is that, it can be easily modified to analyze the non-standard router architecture, just by letting the traversing time of non-existing stages to be zero. Thus, we first obtain the service curve of all these 5 stages.
\begin{figure*}
  \centering
  \subfloat[Service Curve of BW, SA and ST stages]{\includegraphics[scale=0.5]{figures/BW_ST_SA.pdf}\label{result1}}\hspace{10pt}
  \subfloat[Service Curve of RC and VA stages]{\includegraphics[scale=0.5]{figures/RC_VA.pdf}\label{result2}}
  \caption{Service Model for Each Pipeline Stages}
\end{figure*}

(1) BW stage and ST stage: all the flits within a traffic flow will traverse these two stages, and experience a fixed delay $T$. Take the BW stage as an example, because this stage output one flit at each cycle $T$. For any time interval of length less than $T$, the maximum and minimal number of flits that can be seen are one and zero, respectively. Similarly, for any time interval of length greater than $T$, the maximum and minimal number of flits that can be seen are two and one, respectively. The resulted upper and lower service curve are shown in Fig. \ref{result1}.

(2) RC stage and VA stage: only the head flit should traverse these two stages. Suppose the packet length is $L$, a sophisticated solution to construct service curve for these two stages is that, since the traverse latency for the non-head flit is $0$ cycle, we can view these two stages can provide service for all the $L$ flits within $T$ period. Then, we can get the equivalent service curve for these two stages with the same method as BW and ST stages, as shown in Fig. \ref{result2}.

(3) SA stage: each output port in the wormhole-switched NoC has a SA scheduler to schedule the switch traversal at each cycle. Thus, following the same approach as BW and ST stage, we can get the service curve provided by SA stage to all the contention flows, as shown in Fig. \ref{result1}. For the fixed-priority based scheduling policy, each switch allocator provides service for high priority flows first, and flows with the same priority will be served with Round-Robin order. The unserved flits will be imposed an additional latency $T$ due the failure of switch arbitration.

The equivalent service curve for the entire router can be obtained by concatenating all the five service curves together. To obtain a concise notation, denote by the upper service curve and lower service curve provided by Router $R_i$ to flow $f_j$ are $\beta_{R_i,f_j}^u$ and $\beta_{R_i,f_j}^l$, then
$$\beta_{R_i,f_j}^l=\beta_{BW}^l\otimes\beta_{RC}^l\otimes\beta_{VA}^l\otimes\beta_{SA,R_i,f_j}^l\otimes \beta_{ST}^l,$$
$$\beta_{R_i,f_j}^u=\beta_{BW}^u\otimes\beta_{RC}^u\otimes\beta_{VA}^u\otimes\beta_{SA,R_i,f_j}^u\otimes \beta_{ST}^u.$$

The alert readers would notice that, the contention of different flows only occurs at SA stage. If we obtained the service curve provided by SA stage to the target flow $f_j$, we can obtain the service curve of router directly. To obtain the service curve $<\beta_{SA,R_i,f_j}^l,\beta_{SA,R_i,f_j}^u>$, we should consider the following two cases:

(a) all the flows contending with $f_j$ at $R_i$ have lower priorities. Then, for the synchronize router architecture, leftover service curve after serving higher priority flows is provided to $f_j$ totally. But for an asynchronous architecture, the service curve obtained by $f_j$ might be lower than the leftover service curve. For the worst case, flit from $f_j$ arrives at SA stage just after a flit from lower priority gotten granted. Thus, flit from $f_j$ has to stall for a cycle. This is refer to the inference from lower priority. At this circumstance, the final service curve obtained by flow $f_j$ is
\begin{equation}\label{nonpreemptbetal}
\beta^{l}_{SA,R_i,f_j}=\max\{0,\beta^{l^\prime}_{SA,R_i,f_j}-1\}.
\end{equation}
Similarly, a higher priority flow can be blocked for a cycle by flow $f_j$. Thus, the upper service curve obtained by flow $f_j$ is
\begin{equation}\label{nonpreemptbetau}
\beta^{u}_{SA,R_i,f_j}=\min\{\beta^{u}_{SA,R_i,f_j},\beta^{u^\prime}_{SA,R_i,f_j}+1\}.
\end{equation}

(b) there exists some contention flows with the same priority as $f_j$, and the leftover service curve after serving flows with high priority than $f_j$ is $<\beta_{SA,R_i}^{l^\prime},\beta_{SA,R_i}^{u^\prime}>$. Denote by the set of contention flows at router $R_i$ with the same priority as $f_j$ as $\Theta_{R_i,f_j}$, and let $N$ be the number of flows in $\Theta_{R_i,f_j}$. Then, the service curve provided to $f_j$ is $<\lfloor\beta^l_{SA,R_i}/N\rfloor,\lceil\beta^u_{SA,R_i}/N\rceil>$, as shown in Fig. \ref{roundrobin}, since all the flows in $\Theta_{R_i,f_j}$ got service in Round-Robin order. This is slightly different from the conventional continuous time model which is $<\beta^l_{SA,R_i}/N,\beta^u_{SA,R_i}/N>$, this is because our model is flit-level preemptive. After serving by all the flows in $\Theta_{R_i,f_j}$, the leftover service capacity for low priority flows can be obtained by applying Eq.(\ref{betal}) and Eq.(\ref{betau}).
\begin{figure*}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.5]{figures/RoundRobin.pdf}\\
  \caption{Service Curve for N Flow With The Same Priority Under Round-Robin Scheduling Polices}\label{roundrobin}
\end{figure*}

\subsection{Upper Service Curve for Flow Controller}\label{flowcontrol}
In this subsection, we try to break the dependence loop caused by flow control protocol and derive the equivalent service curve, especially the equivalent upper service curve, since the lower service curve has been fully discussed in the field of network calculus \cite{Boudec2001Network}. In fact, for this problem, researchers have proposed several solutions, e.g. fixed-point iteration \cite{schioler2005network}\cite{Jonsson:2008:CDM:1450058.1450083}, transformation from marked dataflow graph \cite{Thiele:2009:MPA:1629335.1629353}. In this paper, we try to tackle the same problem with another solution. This is motivated by \cite{QLDD09FC}, where the authors abstract the flow control as a component providing a service curve, and the equivalent service curve (corresponding to the lower service curve of RTC) of this flow controller was obtained by applying some basic dioid algebra. In this section, we follow the same procedure to derive the upper service curve of flow controller, to make the question clear, we take flow $f_2$ as an example to demonstrate this method. We assume that, the destination NI has sufficient large input buffer, thus there is no flow controller between the router and destination NI. But, to prevent the buffer overflow, there must be a flow controller between input NI and router. We plot the scheduling network of flow $f_2$ in Fig. \ref{f2}, and ignore $f_4$ and the flow control of other flows for simplicity. Denote by the output arrival process of router $R_9$, injection process and departure process of router $R_{5}$ as $A(t)$, $I(t)$ and $D(t)$, respectively. We try to derive the service curve of flow controller at router $R_9$, as shown in the following Theorem.
\begin{figure*}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.35]{figures/f2.pdf}\\
  \caption{Scheduling Network Model for Flow $f_2$}\label{f2}
\end{figure*}

\begin{theorem}\label{credit}
Suppose each router provide an upper service curve $\beta^u$, the virtual channel depth of each router is $B$, and the feedback delay is $T$, then the flow controller provides an equivalent upper service curve $\beta^{u}_\sigma$, where $$\beta^{u}_\sigma(t)=\overline{\beta^u\otimes\delta_T(t)+B}$$
and $\bar{f}$ is the sub-additive closure of $f$, i.e. $\bar{f}=\inf_{n\geq 0}\{f^{(n)}\}$ and $f^{(0)}=\delta_0(t)$\footnote{$\delta_T(t)=+\infty$ for $\forall t>T$, and 0 otherwise.}.
\end{theorem}
\begin{IEEEproof}
The feedback link can be represented as a network element providing upper service curve $\delta_T(t)$. Next, we applying the same idea as \cite{QLDD09FC} to derive the upper service curve this flow controller. We know that, $I(t)=\min\{A(t),D^\prime(t)+B\}$, where $D^\prime=D\otimes\delta_T$. Denote by the service curve of router is $\beta^u$, thus $D(t)\leq I\otimes \beta^u(t)$. Here, we introduce a notation $\wedge$ to simplify our expression, where $a\wedge b=\min\{a,b\}$. Bring $I(t)$ and $D^\prime(t)$ into this equation, we get
\begin{eqnarray*}
D(t)&\leq& I\otimes \beta^u(t)\\
&\leq& A\otimes \beta^u(t)\wedge(D\otimes\delta_T\otimes \beta^u(t)+B).
\end{eqnarray*}
By applying Theorem 4.31 in \cite{Boudec2001Network}, we have
$$D\leq A\otimes \beta^u\otimes\overline{\beta^u\otimes\delta_T+B}.$$
Thus,
\begin{eqnarray*}
  I&=& A\wedge(D^\prime+B)\\
  &\leq& A\wedge(D\otimes\delta_T+B)\\
  &\leq& A\wedge(A\otimes \beta^u\otimes\overline{\beta^u\otimes\delta_T+B}\otimes\delta_T+B)\\
  &=&A\otimes \delta_0\wedge(A\otimes (\beta^u\otimes\delta_T+B)\otimes\overline{\delta_T\otimes\beta^u+B})\\
  &=& A\otimes\overline{\beta^u\otimes\delta_T+B}.
\end{eqnarray*}
which implies that, the flow controller has an equivalent upper service curve $\overline{\beta^u\otimes\delta_T+B}$.
\end{IEEEproof}

Theorem \ref{credit} derives the upper service curve of a single flow controller, and we can get the service curve of a router chain by applying Theorem
\ref{credit} iteratively. As shown in Fig. \ref{f2}, the downstream routers' service curve can affect the service curve of flow controller at upstream. Hence, we should first derive the service curve of $f_2$ obtained at each router with the method mentioned in Subsection \ref{router}, and then compute the service curve of flow controller from destination to source. Service curve obtained at Router $R_{15}$ by flow $f_2$ can be derived by applying Eq.(\ref{betal}) and Eq.(\ref{betau}), since $f_2$ is a lower priority flow at $R_{15}$. Router $R_{13}$, $R_{9}$ and $R_{5}$ provide all their service curves to $f_2$ because $f_2$ is the highest priority flow at these routers. Denote by the service curve of flow controller obtained by Theorem \ref{credit} at router $R_{9}$ is $<\beta_{\tau_9}^l,\beta_{\tau_9}^u>$, by applying the concatenation theorem, we can obtain the equivalent service curve of router $R_{9}$ are $<\beta_{R_9}^l\otimes\beta_{\tau_9}^l,\beta_{R_9}^u\otimes\beta_{\tau_9}^u>$. Then, by applying Theorem \ref{credit}, we can get the service curve of flow controller at router $R_{13}$, $R_{14}$, $R_{15}$ and $IP_{15}$ iteratively.

\subsection{Longest Collapsible Sub-Path}
We have build the traffic model, router model and flow control model in the previous subsection. Before we give the performance evaluation algorithm, we first consider the follow scenario. Take $f_2$ and $f_3$ in Fig. \ref{topology} as an example, they content the output link of both router $R_{13}$ and $R_{9}$. Denote by the buffer depth of $R_{9}$ as $B_{9}$, the service curve of $R_{13}$ and $R_{9}$ be $<\beta_{R_{13}}^l,\beta_{R_{13}}^u>$ and $<\beta_{R_{9}}^l,\beta_{R_{9}}^u>$, the service curve for flow controller of router $R_{13}$ be $<\beta_{\tau_{13},f_2}^l,\beta_{\tau_{13},f_2}^u>$, the arrival curve of $f_2$ and $f_3$ at router $R_{13}$ be $<\alpha_{R_{13},f_2}^l,\alpha_{R_{13},f_2}^u>$ and $<\alpha_{R_{13},f_2}^l,\alpha_{R_{13},f_2}^u>$. The question is: when $B_9$ is sufficiently large so that $\beta_{R_{13}}^l\otimes\beta_{\tau_{13},f_2}^l=\beta_{R_{13}}^l$ and $\beta_{R_{13}}^u\otimes\beta_{\tau_{13},f_2}^u=\beta_{R_{13}}^u$, how to derive the leftover service curve for $f_3$ at $R_{13}$ and $R_9$, and obtain the delay and backlog bound efficiently with tighter bound?

An intuitive solution is that calculating the leftover service curve router by router: firstly, obtaining the leftover service curve of $R_{13}$ by applying Eq.(\ref{betal}) and Eq.(\ref{betau}); secondly, deriving the output arrival curve $<\alpha_{R_{9},f_2}^{l^\prime},\alpha_{R_{9},f_2}^{u^\prime}>$ of $f_2$ by applying Eq.(\ref{alphal}) and Eq.(\ref{alphau}); thirdly, the leftover service curve of $R_9$ can be easily obtained by applying Eq.(\ref{betal}) and Eq.(\ref{betau}); and finally, the leftover service curve for $f_3$ at $R_{13}$ and $R_9$ can be easily obtained by concatenation theorem of service curve. Another solution is: we can substitute $R_{13}$ and $R_{9}$ by a virtual router $R_{13,9}$ providing service curve $<\beta_{R_{13},f_2}^l\otimes\beta_{R_{13},f_2}^l,\beta_{R_{13},f_2}^u\otimes\beta_{R_{13},f_2}^u>$, since $\beta_{R_{13}}^l\otimes\beta_{\tau_{13},f_2}^l=\beta_{R_{13}}^l$ and $\beta_{R_{13}}^u\otimes\beta_{\tau_{13},f_2}^u=\beta_{R_{13}}^u$, as shown in Fig. \ref{collapse}. Then, the leftover service curve $f_3$ obtained at $R_{13}$ and $R_{9}$ can be directly obtained by applying Eq.(\ref{betal}) and Eq.(\ref{betau}). Compared with previous method, this one eliminates the calculation of intermediate arrival curve, and compute the equivalent service curve by just invoke Eq.(\ref{betal}) and Eq.(\ref{betau}) once, which is calculation efficient. We also demonstrate that, this method can achieve tighter performance bound compared with the former method, we will demonstrate this in Section \ref{experiments}.
\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.6]{figures/collapse.pdf}\\
  \caption{Longest Collapsible Sub-Path of $f_2$}\label{collapse}
\end{figure}

Next, we formalize this observation and propose an efficient performance calculation method. This optimization method is based on the follow concept.
\begin{definition}[Longest Collapsible Sub-Path]
The Longest Collapsible Sub-Path of flow $f$, denoted by $P(f)$, is the longest sub-path of $f$ satisfying the follow conditions:
\begin{enumerate}
  \item all the routers in this sub-path, except the last one, satisfy the following condition:
      \begin{itemize}
        \item $\beta_{R_{13}}^l\otimes\beta_{\tau_{13},f_2}^l=\beta_{R_{13}}^l$;
        \item and $\beta_{R_{13}}^u\otimes\beta_{\tau_{13},f_2}^u=\beta_{R_{13}}^u$.
      \end{itemize}
  \item $P(f)$ is also the sub-path of $\Omega_{f}$, where $\Omega_{f}$ is the set of contention flows with lower priority on this sub-path.
\end{enumerate}
\end{definition}

We can replace all the routers on $P(f)$ by a single virtual router providing service curve $<\otimes_{R_i\in P(f)}\beta_{R_i,f}^l,\otimes_{R_i\in P(f)}\beta_{R_i,f}^u>$, and the leftover service curve calculation is simplified. For the high priority flows, the process of calculating the end-to-end equivalent service curve is also a collapsing process. But, to compute the leftover service curve at some routers, we have to know the arrival curve at this router. With the aid of longest collapsible sub-path, we just only to compute the arrival curve at the starting point of each sub-path. For a longest collapsible sub-path with $N$ routers, this method reduces $N-1$ times calculation of arrival curve and leftover service curve, which is time efficient.

\subsection{Calculating the End-to-End Latency}
After obtained the traffic model, router model and flow control model, compute the end-to-end latency is still a non-trivial task. The follow four aspects should be considered carefully: (1) We should always compute the end-to-end latency by collapse the system to a single virtual node, as the hop-by-hop computation will lead to a looser bound; (2) In the fixed-priority flit-level preemptive NoC, only the leftover service curve can be used by the low priority flows, thus, our computation must start from higher priority flows to lower priority; (3) Before computing the leftover service curve for lower priority flows, we must ensure that all the higher priority flows has been served, in our algorithm, the label 'Calculated' is used for this purpose; (4) The computed service curve for flow controller can only be applicable for specific flow, and we should compute these curves for each flows. Keeping these four aspects in mind, we propose the performance evaluation algorithm, as shown in Algorithm. \ref{alg:equivalentservicecurve}.

Here are some explanation about our algorithm: suppose the entire NoC is represented as a directional topology graph $G:\ V\times E$, where $V,E$ represent the router and link respectively. For each link $e_{i,j}\in E$ represent there is a physical channel between router $R_i$ and router $R_j$, because the graph is a directional graph, $e_{i,j}\neq e_{j,i}$. The set of all the flows in the network is denoted as $F$, and the path set of a flow $f_i$ traversed is denoted as $\Gamma_i$. If there exists interference between flow $f_i$ and $f_j$, $\Gamma_i\wedge\Gamma_j=\Phi$. Further, if $e_{k,l}\in\Gamma_i\wedge\Gamma_j$, it means flow $f_i$ and $f_j$ content with each other at router $R_k$. All the flows $f_i$ has a fixed-priority $P_i$. Denote the routers that flow $f_i$ passes as a set $\overline{\mathcal{R}_{f_i}}$, the arrival curve of $f_i$ at router $R_j$ is $<\alpha_{i,j}^u,\alpha_{i,j}^l>$, and router $R_j$ provides a service curve $<\beta_{R_j,f_i}^l,\beta_{R_j,f_i}^u>$ for flow $f_i$. Suppose the buffer size at each router $R_i$ is $B_i$. For all the router $R_j$ along the routine of flow $f_i$, denote by the contention flows with the same priority as $f_i$ as $\Theta_{R_j,f_i}$, and the set with lower priority flow as $\Omega_{R_j,f_i}$.

\begin{algorithm}
\caption{Calculating the End-to-End Latency}
\label{alg:equivalentservicecurve}
\begin{algorithmic}[1]
    \FOR {All flows $f_i\in F$}
        \FOR {All router $R_j\in \overline{\mathcal{R}_{f_i}}$}
            \STATE Let $\beta_{i,j}^u=\beta_{i,j}^l=\delta_0(t)$.
        \ENDFOR
    \ENDFOR
    \FOR {For each flow $f_i\in F$, with priority order}
        \STATE Collapse all the possible sub-path of $f_i$.
        \FOR {each router $R_j\in \overline{\mathcal{R}_{f_i}}$}
            \IF {$\Theta_{R_j,f_i}\neq \emptyset$}
                \FOR {each flow $f_k\in \Theta_{R_j,f_i}$}
                \STATE Compute $<\beta_{SA,R_i,f_k}^{l^\prime},\beta_{SA,R_i,f_k}^{u^\prime}>$;
                \STATE Compute $<\beta_{R_i,f_k}^l,\beta_{R_i,f_k}^u>$;
                \ENDFOR
            \ENDIF
            \IF {$\beta_{i,j}^u=\beta_{i,j}^l=\delta_0(t)$}
                \STATE Compute $\beta_{i,k}$ with Eq.(\ref{nonpreemptbetal}) and Eq.(\ref{nonpreemptbetau}).
            \ENDIF
        \ENDFOR
        \STATE Compute $<\beta_\sigma^l,\beta_\sigma^u>$ with Eq.(\ref{credit}).
        \STATE Compute the service curve $<\beta^l,\beta^u>$ for flow $f_i$.
        \STATE Compute the latency of flow $f_i$ according to Eq.(\ref{delay}).
        \STATE Compute the backlog of flow $f_i$ according to Eq.(\ref{backlog}).
        \STATE Label the flow $f_i$ as 'Calculated'.
        \FOR {For all the router $R_j$ in $\overline{\mathcal{R}_{f_i}}$}
            \IF {$\Omega_{R_j,f_i}\neq \emptyset$ and all the flows in $\Theta_{R_j,f_i}$ have been labeled as 'Calculated'}
                \STATE Compute the arrival curve $<\alpha^l_{i,j},\alpha^u_{i,j}>$ of flow $f_i$ at router $R_j$ according to Eq.(\ref{alphau}) and Eq.(\ref{alphal}).
                \STATE Compute the arrival curve $<\alpha^l_{k,j},\alpha^u_{k,j}>$ at router $R_j$ of all the flows in $\Theta{R_j,f_i}$ according to Eq.(\ref{alphau}) and Eq.(\ref{alphal}), where $f_k\in\Theta{R_j,f_i}$.
                \STATE Compute the aggregative arrival curve at $R_j$ of  flow $f_i$ and flows in $\Theta{f_i,j}$.
                \STATE Compute the leftover service curve $<\beta^{l^\prime}_{i,j},\beta^{u^\prime}_{i,j}>$ at router $R_j$ after serving flow $f_i$ and all flows in $\Theta_{R_j,f_i}$ according to Eq.(\ref{betal}) and Eq.(\ref{betau}).
            \ENDIF
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}

In algorithm \ref{alg:equivalentservicecurve}, the leftover service curve for lower priority flows should compute after all the service curve of higher priority flows have been calculated, we add the label 'Calculated' to distinguish this scenario. The overall algorithm has two embedded loops, and the computation complexity for this algorithm is $O(np)$, where $n$ and $p$ is the number of flows and the hop count of each flow. This algorithm is of pseudo-polynomial complexity due to the computation complexity of algorithmic min-plus convolution and sub-additive closure \cite{Bouillard2008}. This algorithm can be easily integrated into the network calculus toolbox, such as disco\cite{5755058} and RTC toolbox\cite{rtc} to compute the end-to-end latency automatically.

\subsection{Buffer Optimization}
The priority-aware NoC requires the same amount of VC as the priorities, to reduce the design cost, a priority sharing techniques is proposed in \cite{5161497}. We observed that, for the priority-aware NoC, the lower priorities, the higher locking ratios, thus leading to larger buffer size. In this section, we propose a buffer optimization algorithm in conjunction with the priority sharing techniques to reduce the cost of priority aware NoC. Our method can be applied to the application-specific NoC design. We assume the application has been mapped to the NoC, and different flows have been assigned to their corresponding priorities and deadline. Our aim is the reduce the buffer size under the constraint of deadline not be violated.

The path of flow $f_i$ is a sequence of router, denote by the start point $start_i$, end point $end_i$, and for any router $curnode$ along the path, Denote by $Pre(curnode)$, and $Pre(start_i)=null$. Initially, let the buffer size of $R_j$ reserved for $f_i$ be $B_{j,i}=\inf\{B\geq 0|\beta_{\tau}\otimes\beta_{R_i}\geq \beta_{R_i}\}$, which is the smallest buffer size which disable the flow control. After assigned the initial buffer size for each flow along the path, we optimize the buffer size by the follow way: for each flow from high priority to low priority, e.g. flow $f_i$, reduce the buffer size gradually from $end_i$ to $start_i$, for each iteration, verify that whether the deadline is satisfied. Move $curnode$ to its predecessor $Pre(curnode)$ if the former reduction violates the deadline constraint. Iteration for flow $f_i$ ends when $pre(curnode)=null$. The detailed algorithm is shown in Algorithm \ref{alg:bufopt}:
\begin{algorithm}
\caption{Buffer Optimization Algorithm}
\label{alg:bufopt}
\begin{algorithmic}[1]
    \FOR {All flows $f_i\in F$}
        \STATE Compute $\beta_{R_j,f_i}$ and $\beta_{\tau_j,f_i}$;
        \STATE Let $B_j=\inf\{B\geq 0|\beta_{\tau}\otimes\beta_{R_i}\geq \beta_{R_i}\}$;
    \ENDFOR
    \FOR {All flows $f_i\in F$}
        \STATE curnode=$end_i$;
        \WHILE {$Pre(curnode)\neq null$}
            \WHILE {$f_i$ meets its deadline}
                \STATE Reduce $B_{curnode,i}$ by 1;
                \STATE Recalculate the end-to-end deadline of $f_i$;
            \ENDWHILE
            curnode=Pre(curnode);
        \ENDWHILE
    \ENDFOR
\end{algorithmic}
\end{algorithm}

The entire algorithm optimize the buffer size for each flow from high priority to low priority gradually, the entire procedure is also try to reduce the service capacity for high priority, which left more service to the flow priority flows. The entire computation complexity is $O(np)$, where $n$ and $p$ are the number of flows and the number of router along the path. This complexity is pseudo-polynomial due to the end-to-end latency calculation. We can also halt the optimization when the buffer budget are met to reduce the computation procedure. Flow-Level Buffer Analysis and Link-Level Buffer Analysis are proposed to optimize the buffer size for priority aware NoC in \cite{189}, but all these two method assume the packet injection period is less than the end-to-end latency, which limit the feasibility of these methods. In addition, a network calculus based latency model was proposed and energy optimization based on this method was carried out with Dynamic Voltage and Frequency Scaling (DVFS) in \cite{6560630}. Our buffer optimization algorithm can also be used to minimize the buffer consumption and chip area, since the buffer usually consumes about 30\% power of entire chip.  The difference between \cite{6560630} and our method is that: for the fixed configuration and deadline, they minimize the energy consumption by adjusting the voltage and frequency, and our method try to optimize the buffer size to meet the deadline constraint.


\section{Experiments}\label{experiments}
\subsection{Simulation Setup}
In this section, we will utilize the Heterogenous-Networks-on-Chip (HNoCs) \cite{6404157}, a modular open source NoC simulator, to verify our analytical model. To achieve this goal, we modify the switch allocator to support fixed-priority scheduling, and collect the maximal end-to-end packet delay at the destination IP core. We configure the clock cycle to be $2ns$, and the topology is $4\times 4$ mesh. Suppose all the flows $f_i,(i=1,2,3,4)$ in the network are periodical sources, and the period is $P_i$. Denote by the packet length of flow $f_i$ is $L_i$ (in flits), which means that, packets from the same flow has the same length, but packets of different flows can be of different lengths. We can obtain the arrival curve according to the method introduced in subsection \ref{traffic}.

\subsection{Comparison with Other Methods and Simulation}
There are lots of latency analysis models for wormhole-switched real-time communication, e.g. contention tree model \cite{LuJS05}, FLA model \cite{Shi:2008:RCA:1397757.1397996}, LLA model \cite{73}, network calculus model \cite{Qian489900}, there are also lots of backlog analysis models, e.g. scheduling analysis model \cite{Manolache:2006:BSO:1131481.1131683} and LLA model \cite{189}, in addition, the network calculus model proposed in \cite{Qian489900} can also be used to analyze the backlog bound. In this section, we only compare our model with LLA and network calculus, as they outperform all the other existing approach.

\subsubsection{Comparison with Link Level Analysis}
The original LLA approach assumes each flit in a packet traverse the router with a fixed latency of 1 cycle, and can only be applicable the scenario that the deadline of each packet is less than its injection period, which limits its application. To compare with our method, we should specialize the standard 5 stages wormhole router to a single cycle router, this is achieved by letting the latency of BW, RC, VA and ST stage be zero. Thus, the service curve of entire router is the same as the service curve provided by the SA stage, which is $<\beta_{SA,R_i}^l,\beta_{SA,R_i}^u>$. We also have to suppose the VC buffers are large enough, so that we can apply LLA for the latency analysis. For this scenario, let the packet length be 1 flit, we calculate end-to-end latency for each packet with our method and LLA under different injection period. There are two flows sharing the same priority, i.e. $f_1$ and $f_4$. While analyzing the latency of $f_1$, we can treat $f_4$ as a flow with higher priority than $f_1$, and vice versus. the calculation result is shown in Table \ref{LLAvsRTC}. The author can verify these results by hand. We also need to mention that, the RTC result is obtained by considering the longest collapsible sub-path (i.e. $R_{13}$ and $R_{9}$). If this is not take into consideration, then, the analytical result for $f_1$, $f_2$, $f_3$ and $f_4$ are $5,6,7,5$, respectively.
\begin{table}[htbp]
\centering
\caption{\label{LLAvsRTC}Comparison with Link Level Analysis}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Injection Rate}  & \multicolumn{4}{|c|}{Link Level Analysis} & \multicolumn{4}{|c|}{Real Time Calculus} \\
\cline{2-9}
& f1 & f2 & f3 & f4 & f1 & f2 & f3 & f4\\
\hline\hline
2 cycle & \multicolumn{4}{|c|}{$\infty$} & 5 & 8 & 9 & 5\\
\hline
3 cycle & \multicolumn{4}{|c|}{$\infty$} & 5 & 7 & 8 & 5\\
\hline
4 cycle & \multicolumn{4}{|c|}{$\infty$} & 5 & 6 & 6 & 5\\
\hline
5 cycle & \multicolumn{4}{|c|}{$\infty$} & 5 & 6 & 6 & 5\\
\hline
6 cycle & 5 & 6 & 6 & 5 & 5 & 6 & 6 & 5\\
\hline
7 cycle & 5 & 6 & 6 & 5 & 5 & 6 & 6 & 5\\
\hline
\end{tabular}
\end{table}

\subsubsection{Comparison with Network Calculus and Simulation}
In this Subsection, we present the numerical results and simulation results to demonstrate the tightness and improvement of our method. In \cite{Qian489900}, the authors derived the service curve for each contention flow at the same input router. But, these service curves can only be used to derive the latency of injection router, for the service curve of each flow at subsequent router, the service curve should be computed with the same procedure as \cite{qian2009analysis}. We set the flit injection rate as $0.1$ flit/cycle, buffer depth as $B=4$ flits, and change the packet length from $1$ flit to $16$ flits. The simulation results and numerical results is plotted in Fig. \ref{comparison}. By comparison, we find that, for flow $f_1$ and $f_2$, the network calculus method and our method get the same latency result. But, for flow $f_3$, we find that, RTC can derive a much tighter latency bound compared with network calculus, as shown in Fig. \ref{comparison}. and the difference becomes larger when we increase the packet length. One important reason is that, the simulation might not cover the corner case during the simulation process.
\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.6]{figures/comparison.pdf}\\
  \caption{Comparison with Network Calculus and Simulation}\label{comparison}
\end{figure}

Next, we explain the reason why our method outperform the network calculus based method proposed in \cite{Qian489900}. As implied by Theorem 1.72 in \cite{Boudec2001Network}, if we can take the maximal service curve into consideration, we can get a much tighter output arrival curve, then, the leftover service curve of the next router will be much tighter than the one calculated with \cite{qian2009analysis}. To demonstrate this, let $B=4$, $L=4$ flits and $T=2ns$, we obtained the calculated service curve for flow $f_2$ both with network calculus and RTC, as shown in Fig. \ref{loose}. The calculation was carried out with RTC toolbox. From Fig. \ref{loose}, we find that, the calculated service curve with RTC is much 'better' than network calculus, which explains why the RTC can produce better results.
\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.6]{figures/loose.pdf}\\
  \caption{Comparison of Service Curve Computed with Real-Time Calculus and Network Calculus}\label{loose}
\end{figure}

\subsubsection{Buffer Optimization Comparison}
As has shown in previous subsections, the network calculus based method \cite{Qian489900} can handle some circumstances that FLA and LLA can not be applied. This motivate us to optimize the buffer size of NoC with our method. Let the flit injection rate be $0.1$ flit/cycle, packet length $L=16$ flits and change the VC buffer depth from 4 flits to 10 flits, we compare the theoretical bound computed with network calculus and our method, as shown in Fig. \ref{buffer}. It clear that, our method outperform the network calculus based method, because it gives much tight bound for both $f_2$ and $f_3$. Further, if we set the deadline of $f_2$ and $f_3$ to 150ns, with our method, we can found that $B=8$ flits is sufficient enough to guarantee the delay bound. While the network calculus based estimation is larger than $10$ flits. We also observed from Fig. \ref{buffer} that, the high priority flows (i.e. $f_1$ and $f_4$) are very insensitive to the buffer depths, which motivate us to allocate small buffers for these flows to reduce the area and power cost. Link-level analysis was extended in \cite{189} to finite buffer size scenario. But, we need to mention that, the LLA approach can not be applied to optimize the buffer size, this is because the network latency is larger than the injection period for this configuration.
\begin{figure*}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.5]{figures/buffer.pdf}\\
  \caption{end-to-end flows with different buffer size}\label{buffer}
\end{figure*}

\section{Conclusion}\label{conclusion}
The priority-aware wormhole-switched NoC is a promising platform for the on-chip real-time communication if the worst-case performance can be accurately analyzed and guaranteed. In this paper, we derived the upper service curve of flow controller with a novel method. Our contribution in this paper is two folded: First, we proposed a RTC based performance model for this kind of NoC, which outperforms the conventional scheduling theory and network calculus based methods. We also propose the concept of longest collapsible sub-path to improve the calculation accuracy and reduce the computation complexity; Second, we also propose an RTC based buffer optimization algorithm to reduce the buffer size under the constraint of deadline imposed to each flows. Our results can be applied to the mapping, routing and power optimization of NoC.

\section*{Acknowledgement}
The authors would thank the reviewers for their suggestions and comments, and all the experiments are carried out at VLSI Lab of McGill University. This research is supported by High Technology Research and Development Program of China (Grant No. 2012AA012201, 2012AA011902).

\bibliographystyle{unsrt}
\bibliography{Docear}

\end{document} 